{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057f4bbf",
   "metadata": {},
   "source": [
    "**Entrenamiento**\n",
    "\n",
    "El entrenamiento de las Temporal Neural Networks (TNN) consiste en ajustar los parámetros del modelo para que la red aprenda relaciones que dependen del tiempo presente en la secuencia de datos. A diferencia de las redes neuronales tradicionales, en una TNN la salida no depende únicamente de la entrada actual, sino también de entradas pasadas o de estados internos previos, lo que introduce un componente temporal en el proceso de aprendizaje.\n",
    "\n",
    "El método de entrenamiento utilizado depende directamente de la arquitectura temporal de la red. En términos generales, los enfoques más comunes se dividen en:\n",
    "\n",
    "**- Redes con retados temporales**\n",
    "\n",
    "**- Redes con estados recurrentes**\n",
    "\n",
    "Cada uno de estos enfoques incorpora el tiempo de manera distinta y, por tanto, requiere estrategias de entrenamiento diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a9dcf",
   "metadata": {},
   "source": [
    "**Redes con retados temporales**\n",
    "\n",
    "Las TNN basadas en retardos temporales, como las Time-Delay Neural Networks (TDNN), incorporan la información temporal expandiendo la entrada con valores pasados dentro de una ventana temporal fija.\n",
    "\n",
    "En palabras más sencillas, estas redes funcionan “mostrándole” al modelo no solo lo que está ocurriendo en el instante actual, sino también lo que ocurrió hace unos momentos, de esta forma, la red puede identificar patrones que se desarrollan a lo largo de varios instantes de tiempo consecutivos.\n",
    "\n",
    "**En este tipo de arquitectura:**\n",
    "\n",
    "- La memoria es explícita, ya que los valores pasados se incluyen directamente como parte de la entrada.\n",
    "\n",
    "- No existen estados internos que se mantengan entre pasos temporales.\n",
    "\n",
    "- El entrenamiento se realiza mediante backpropagation estándar, ya que la red se comporta como una red feedforward tradicional.\n",
    "\n",
    "- Este enfoque es computacionalmente eficiente y adecuado para problemas donde las dependencias temporales se encuentran dentro de un intervalo de tiempo limitado.\n",
    "\n",
    "**Redes con estados recurrentes**\n",
    "\n",
    "Cuando las TNN poseen estados internos recurrentes, la red mantiene una forma de memoria interna que se actualiza en cada paso temporal. En este caso, la información del pasado no se introduce explícitamente en la entrada, sino que queda almacenada en los estados internos del modelo.\n",
    "\n",
    "El entrenamiento de estas redes se realiza mediante el algoritmo conocido como Backpropagation Through Time (BPTT). Este método consiste en “desenrollar” la red a lo largo del tiempo, transformándola conceptualmente en una red más profunda donde cada capa representa un instante temporal diferente.\n",
    "\n",
    "Dicho de forma más sencilla, cuando una TNN tiene memoria interna, entrenarla implica “estirar” la red a lo largo del tiempo, como si se copiara la misma red varias veces una detrás de otra. Esto permite calcular cómo las decisiones tomadas en instantes anteriores influyen en los resultados posteriores y ajustar los parámetros considerando el historial completo de la secuencia.\n",
    "\n",
    "Este tipo de entrenamiento permite modelar dependencias temporales más complejas y de largo alcance, aunque suele ser más costoso en términos computacionales.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e864ad",
   "metadata": {},
   "source": [
    "En conclusión, el entrenamiento de las TNN depende del mecanismo temporal incorporado en la arquitectura. Mientras que las redes con retardos temporales utilizan estrategias de entrenamiento más simples basadas en ventanas fijas, las redes con estados recurrentes requieren métodos especializados como BPTT para aprovechar su capacidad de memoria dinámica."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
