{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ee02e6",
   "metadata": {},
   "source": [
    "**-- Arquitectura --**\n",
    "\n",
    "La arquitectura de las Temporal Neural Networks (TNN) se caracteriza por incorporar la dimensión temporal de manera explícita dentro del modelo neuronal. Esto significa que la salida producida en un instante de tiempo t no depende únicamente de la entrada actual, sino también de entradas anteriores o de estados previos del propio sistema.\n",
    "\n",
    "Gracias a esto, las TNN pueden modelar fenómenos dinámicos en los que la evolución en el tiempo es un factor determinante, a diferencia de las redes neuronales tradicionales que procesan cada entrada de forma independiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5afe1",
   "metadata": {},
   "source": [
    "**-- Componentes fundamentales de una TNN --**\n",
    "\n",
    "**- Entradas temporales**\n",
    "\n",
    "Las entradas a una TNN no corresponden a un único vector de datos, sino a una secuencia temporal de vectores. Esta secuencia permite que la red capture patrones que dependen tanto del orden como de la duración de los eventos en el tiempo, lo cual es esencial en problemas donde el contexto temporal influye directamente en el resultado.\n",
    "\n",
    "**- Ventanas temporales y retardos**\n",
    "\n",
    "Una de las arquitecturas más simples de las TNN utiliza retardos temporales en las entradas, entendidos como el mecanismo mediante el cual la red no procesa únicamente la entrada actual, sino que también incorpora valores del pasado dentro de una ventana temporal fija.\n",
    "\n",
    "Este enfoque fue introducido formalmente en las Time-Delay Neural Networks (TDNN), donde cada neurona puede procesar información proveniente de múltiples instantes de tiempo, permitiendo identificar patrones locales en secuencias temporales sin necesidad de mantener estados internos.\n",
    "\n",
    "\n",
    "**- Estados internos y memoria dinámica**\n",
    "\n",
    "Estructuras más generales introducen estados internos en su funcionamiento, los cuales almacenan información del pasado y se actualizan en cada paso temporal. En este tipo de arquitecturas, la salida de la red depende tanto de la entrada actual como del estado interno previo, lo que permite modelar dependencias temporales más largas y complejas.\n",
    "\n",
    "Estas arquitecturas dotan a la red de una forma de memoria dinámica, gracias a la cual es posible capturar la evolución de un sistema a lo largo del tiempo sin limitarse a una ventana temporal fija.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
